# Linear Algebra, Singular Value Decomposition


SVD
+ dimension reduction, compressing, denoising, data reduction, ect
+ least squares linear regression, etc. 

https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/

A가 임의의 $m \times n$ 행렬이며, rank가 k라 하자. 즉, A를 RREF 꼴로 만들었을 때, pivot 개수가 k개이며, linearly independent column 개수가 k개 있다는 말이다. 또한, A의 singular value (특이값)들이 순서대로 정렬했을 때, $\sigma\_{1}, \sigma\_{2} ... \sigma\_{k}$라고 하자. 이 말은 곧, $A^{\top} A$의 eigenvector들인 $\lambda\_{1}, \lambda\_{2}, \cdots \lambda\_{k}$들에 대해서, $\sigma\_{i}=\sqrt{\lambda\_{i}}$ for $i \in[1, k]$ 인 $\sigma\_{i}$들의 집합들을 A의 singular value들이라고 한다. 물론, $\sigma\_{1}, \sigma\_{2} ... \sigma\_{k}$ 이 크기 순으로 정렬되어 있지만, 이 중에서 중복값이 나올 수 있다. 


$$\operatorname{rank}(A)=k$$

$$
\sigma\_{1} \geq \sigma\_{2} \geq \cdots \geq \sigma\_{k}>0
$$

그렇다면 Rn과 Rm에 대해서, 각각 공간의 orthonormal basis인 B1과 B2가 존재하며, B1과 B2는, 위에서 정의한 singular value들에 대해서, 다음과 같은 두 관계식을 가진다. 

$$
\begin{aligned}
B\_{1} &=\left\{\vec{v}\_{1}, \vec{v}\_{2}, \ldots \vec{v}\_{n}\right\} \text { for } \mathbb{R}^{n} \\
B\_{2} &=\left\{\vec{u}\_{1}, \vec{u}\_{2}, \ldots \vec{u}\_{m}\right\} \text { for } \mathbb{R}^{m}
\end{aligned}
$$

$$
\left[A \vec{v}\_{1}\left|A \vec{v}\_{2}\right| \ldots\left|A \vec{v}\_{k}\right| A \vec{v}\_{k+1}|\cdots| A \vec{v}\_{n}\right] = \left[\sigma\_{1} \vec{u}\_{1}\left|\sigma\_{2} \vec{u}\_{2}\right| \ldots\left|\sigma\_{k} \vec{u}\_{k}\right| \vec{0} \ldots \vec{0}\right] \tag{4}
$$

$$
\left[A^{\top} \vec{u}\_{1}\left|A^{\top} \vec{u}\_{2}\right| \ldots\left|A^{\top} \vec{u}\_{k}\right| A^{\top} \vec{u}\_{k+1}|\ldots| A^{\top} \vec{u}\_{m}\right] = \left[\sigma\_{1} \vec{v}\_{1}\left|\sigma\_{2} \vec{v}\_{2}\right| \ldots\left|\sigma\_{k} \vec{v}\_{k}\right| \vec{0} \ldots \vec{0} \right]
$$

따라서, 행렬 V를 B1 속의 벡터들을 열벡터로 갖는 행렬로 정의하고, 행렬 U를 B2 속의 벡터들을 열벡터로 가지는 행렬로 정의하자. 그러면 V는 $n \times n$ 행렬이며, U는 $m \times m$ 행렬이다. 또한, V와 U의 열벡터들이 orthonormal한 basis이다 보니, U와 V는 orthogonal 행렬이다.   

$$
V=\left[\vec{v}\_{1}\left|\vec{v}\_{2}\right| \ldots \mid \vec{v}\_{n}\right]
$$

$$
U=\left[\vec{u}\_{1}\left|\vec{u}\_{2}\right| \ldots \mid \vec{u}\_{m}\right]
$$

그리고 위에서 구한 singular value들에 대해서, $\Sigma$라는 $m \times n$ 행렬을 다음과 만들자.

$$
\Sigma=\left[\begin{array}{cccc|cccc}
\sigma\_{1} & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & \sigma\_{2} & & 0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma\_{k} & 0 & 0 &  & 0 \\
\hline 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\ 
\end{array}\right]
$$

(4)번 식을 다시 보자. (4)번 식을 좀더 간결하게 작성해보면, 다음과 같이 쓸 수 있다. 

$$
\begin{aligned}
AV &=\left[A \vec{v}\_{1}\left|A \vec{v}\_{2}\right| \ldots A \vec{v}\_{n}\right] \\
&=\left[\sigma\_{1} \vec{u}\_{1}|\ldots| \sigma\_{k} \vec{u}\_{k} \mid \vec{0} \ldots \vec{0} \right] \\
\end{aligned}
$$

하지만 위에서 정의한 $\Sigma$ 행렬을 사용하자면, 바로 위에서 작성한 식을 다음과 같이 바꿔쓸 수 있다. 

$$
\begin{aligned}
AV&=\left[\vec{u}\_{1}|\ldots| \vec{u}\_{m}\right] \Sigma \\
&=U \Sigma \\
\end{aligned}
$$

그리고 이제 본격적으로 우리가 흔히 아는 SVD의 본모습이 나오는데, V가 orthonormal basis of Rn이므로, orthonormal의 정의에 따라서, $V^{-1}$가 존재하며, $V^{-1} = V^\top$이다. 따라서, 다음과 같이 바꿔쓸 수 있으며, 우리가 흔히 알고있는 SVD 공식의 모습이다. 

$$
\therefore A V=U \Sigma \;\; \longrightarrow \;\; A=U \Sigma V^{-1}=U \Sigma V^{\top}
$$
