---
title: "SVD, PCA"
date: 2022-03-01
toc: true
mathjax: true
categories:
  - study
tags:
  - SVD
  - PCA
---



---

# Singular Value Decomposition


Sources: 

1. [Math LibreTexts on S.V.D](https://math.libretexts.org/Bookshelves/Linear_Algebra/Book%3A_Matrix_Analysis_(Cox)/11%3A_Singular_Value_Decomposition/11.01%3A_The_Singular_Value_Decomposition)
2. Wikipedia
3. Gil Strang [Lecture](https://www.youtube.com/watch?v=rYz83XPxiZo)

Let A be m by n matrix of rank k. There exists orthonormal bases $B_{1}, B_{2}$ and scalars $\{\sigma_1 ... \sigma_k\}$

$$
\begin{aligned}
B_{1} &=\left\{\vec{v}_{1}, \vec{v}_{2}, \ldots \vec{v}_{n}\right\} \text { for } \mathbb{R}^{n} \\
B_{2} &=\left\{\vec{u}_{1}, \vec{u}_{2}, \ldots \vec{u}_{m}\right\} \text { for } \mathbb{R}^{m} 
\end{aligned}
$$

$$\sigma_1 \dots \sigma_k \in \mathbb{R}, \;\; (\sigma_1 > \sigma_2 > \dots > \sigma_k > 0)  $$

such that 

$$
\left\{\begin{array}{ll} A \vec{v}_i = \sigma_i \vec{u}_i & (1 \leq i \leq k) \\ A^\top \vec{u}_i = \sigma_i \vec{v}_i  & (1 \leq i \leq k) \end{array}\right.
$$

Define matrix V and U as the following. 

$$
V = [\vec{v}_1 | \vec{v}_2 | ... \vec{v}_n] \\
U = [\vec{u}_1 | \vec{u}_2 | ... \vec{u}_m]
$$ 

$$$$

$$
\Sigma=\left[\begin{array}{cccc|cccc}
\sigma_{1} & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & \sigma_{2} & & 0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{k} & 0 & 0 &  & 0 \\
\hline 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\ 
\end{array}\right]
$$


다음을 보자. 

$$
\begin{aligned}
AV &=\left[A \vec{v}_{1}\left|A \vec{v}_{2}\right| \ldots A \vec{v}_{n}\right] \\
&=\left[\sigma_{1} \vec{u}_{1}|\ldots| \sigma_{k} \vec{u}_{k} \mid \vec{0} \ldots \vec{0} \right] \\
\end{aligned}
$$

하지만 위에서 정의한 $\Sigma$ 행렬을 사용하자면, 바로 위에서 작성한 식을 다음과 같이 바꿔쓸 수 있다. 

$$
\begin{aligned}
AV&=\left[\vec{u}_{1}|\ldots| \vec{u}_{m}\right] \Sigma \\
&=U \Sigma \\
\end{aligned}
$$

그리고 이제 본격적으로 우리가 흔히 아는 SVD의 본모습이 나오는데, V가 orthonormal basis of Rn이므로, orthonormal의 정의에 따라서, $V^{-1}$가 존재하며, $V^{-1} = V^\top$이다. 따라서, 다음과 같이 바꿔쓸 수 있으며, 우리가 흔히 알고있는 SVD 공식의 모습이다. 

$$
\therefore A V=U \Sigma \;\; \longrightarrow \;\; A=U \Sigma V^{-1}=U \Sigma V^{\top}
$$



---

# Principal Component Analysis

Sources:

1. [Steve Brunton Lecture](https://www.youtube.com/watch?v=fkf4IBRSeEc)
2. Wikipedia on [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis#Computing_PCA_using_the_covariance_method)

간단하게 covariance matrix를 통해서 PCA를 구하는 과정을 소개해보자. 다음과 같이 u라는 벡터가 있다면, 

$$ \begin{align} \mathbf{u} &= \begin{bmatrix} u_{1} \\ u_{2} \\ \vdots \\ u_{m} \end{bmatrix} \end{align} $$

u의 각 element들의 평균들로 이루어진 벡터를 다음과 같이 쓸 수 있다. 

$$\begin{align} \mathbf{\bar{u}} &= \begin{bmatrix} \bar{u} \\ \bar{u} \\ \vdots \\ \bar{u} \end{bmatrix} \end{align}
$$

벡터 x와 x bar에 대해서, 벡터 x의 각 원소들의 표준편차를 s라고 할 때, 벡터 z를 구할 수 있다. 이때 벡터 z는 벡터 원소들의 평균이 0, std가 1인 scaled variable이다.  

{% raw %}
$$\begin{align} \text{scaled variable} \;\; \mathbf{t} = \frac{\mathbf{x} - \mathbf{\bar{x}}}{{s_{x}}} \end{align}$$
{% endraw %}

데이터 표에서 각 variable을 column으로 두고, 이 variable를 벡터로 표현한다면, 다음과 같은 식으로, 벡터로 표현한 각 variable을 scaled variable인 $z_{i}$로 나타낼 수 있다. 

$$ \begin{align} t_{i} = \frac{\vec{x}_{i} - \bar{x_{i}}}{s_{x_{i}}} \end{align} \;\; (i = 1,2,3,4) $$

다음과 같이 정의한 Z 행렬에 대해서, Z행렬의 correlation matrix인 $C_{0}$을 다음 공식으로 구해보자. 

$$T = [t_{1}, t_{2}, t_{3}, t_{4}]$$

$$ \begin{align} C = \frac{1}{m-1} \cdot {T}^\top T \end{align}
$$

그러면 C가 symmetric matrix이므로, C의 eigenvalue들이 diagonal element인 diagonal matrix D가 존재하고, C의 eigenvector들이 column인 eigenvector matrix P가 존재하며, C를 다음과 같이 eigenvector decomposition할 수 있다. 

$$\begin{align} \exists \;\; P, D \;\; \text{s.t.} \;\; C = PDP^{-1} = PD{P}^\top \end{align} $$

여기서 C의 eigenvalue 중, 가장 큰 값의 eigenvalue에 해당하는 eigenvector를 $\vec{u}_{1}$라고 해보자. 그러면 위에서 정의한 행렬 Z에 대해서, first principal component를 $y_1$으로 정의하자. 

$$  \vec{y}_{1} = T \cdot \vec{u}_{1}   $$

비슷한 원리로, second, third, fourth principal component를 구할 수 있다. 이들은 second, third, fourth largest eigenvalue에 해당하는 eigenvector들을 Z에 dot product하면 된다. 



---


