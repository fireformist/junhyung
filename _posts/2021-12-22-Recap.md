title: "Recap"
date: 2021-12-22
toc: true
categories:
  - study
tags:
  - Keras
  - Tensorflow


# Recap

Over the past three months, I've been hesitant to upload new contents on this blog for a variety of reasons, including, but not limited to, working as an instructor at a private math academy, and trying to catch up to what has otherwise been a very busy semester. In particular, while attempting to complete a CS project for one of my classes (building an RNN-based fake news classifier), I had to dig through the contents of the highly accaimed [CS224n](http://web.stanford.edu/class/cs224n/) course, learning about RNNs and language models from scratch. Nonetheless, after reviewing CS224n lecture materials for a whole semester, I came to two conclusions: one, my supposed understanding of core theories/practices of the NLP field fell short of the rigorous emphasis placed on the fundamentals of NLP models/inner workings by CS224n, and two, I needed more practice with certain topics like matrix calculus. 

Until now, I didn't post any materials related to RNNs and language models, but after realizing just how much more there was for me to learn about them, I decided to make several posts, concerning specific topics in NLP like language modeling, attention, word embedding models, and etc. I hope that this effort can serve as the foundational stepping-stone for more advanced topics in the future.  
